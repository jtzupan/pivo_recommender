{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pivo Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Phrase Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrase modeling is an approach to learning combinations of tokens that together represent meaning multi-word concepts.  These phrase models are developed by looping over the words in the corpus and finding words that appear together more than they should by random chance.  The formula used to determine if whether two tokens $A$ and $B$ constitute a phrase is:\n",
    "\n",
    "$$\n",
    "\\frac {count(A B) - count_{min}}\n",
    "{count(A) * count(B)}\n",
    "*N > threshold\n",
    "$$\n",
    "\n",
    "where:\n",
    "* $count(A)$ is the number of times $A$ appears in the corpus\n",
    "* $count(B)$ is the number of times $B$ appears in the corpus\n",
    "* $count(AB)$ is the number of times $AB$ appear in the corpus in that order\n",
    "* $N$ is the total size of the corpus vocabulary\n",
    "* $count_{min}$ is a user-defined parameter to ensure that the phrase appears a minimum number of times\n",
    "* $threshold$ is a user-defined paramter to control how strong the relationship must be before the two tokens are considered a single concept\n",
    "\n",
    "Once we have trained the phrase model we can apply it to the reviews in our corpus.  It will consider the multiworded tokens to be single phrases.\n",
    "\n",
    "The gensim library will help us with phrase modeling, specifically the Phrases class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%mv beerAdvocateScraper/beerAdvocateScraper/BeerAdvocateReviews.csv ./intermediate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile('^[\\d,\"]+')\n",
    "\n",
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            try:\n",
    "                yield re.split(pattern, review)[1].replace('\\\\n', '\\n')\n",
    "            except: pass\n",
    "            if count % 1000 == 0:\n",
    "                print(f'on review {count}')\n",
    "            count += 1\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=1000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intermediate_directory = os.path.join('.', 'intermediate')\n",
    "review_txt_filepath = os.path.join(intermediate_directory,'BeerAdvocateReviews.csv')\n",
    "ngram_all = os.path.join(intermediate_directory, 'ngram_all')\n",
    "unigram_sentences_filepath = os.path.join(ngram_all,\n",
    "                                          'unigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `lemmatized_sentence_corpus` generator to loop over the original review text, segmenting the reviews into individual sentences and normalizing the text.  We will write this data back out to a new file (`unigram_sentence_all`), with one normalized sentence per line.  We will use this data for learning our phrase models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on review 0\n",
      "on review 1000\n",
      "on review 2000\n",
      "on review 3000\n",
      "on review 4000\n",
      "on review 5000\n",
      "on review 6000\n",
      "on review 7000\n",
      "on review 8000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-326bb38bdf16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munigram_sentences_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf_8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlemmatized_sentence_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_txt_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-44e9d771cc81>\u001b[0m in \u001b[0;36mlemmatized_sentence_corpus\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     for parsed_review in nlp.pipe(line_review(filename),\n\u001b[0;32m---> 34\u001b[0;31m                                   batch_size=10000, n_threads=4):\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparsed_review\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, texts, as_tuples, n_threads, batch_size, disable, cleanup)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0moriginal_strings_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mnr_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.get_batch_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mfeature_extracter_fwd\u001b[0;34m(docs, drop)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_feats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uint64'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfeature_extracter_bwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0md_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_feats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uint64'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfeature_extracter_bwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0md_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mget_feats\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_feats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'to_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "    for sentence in lemmatized_sentence_corpus(review_txt_filepath):\n",
    "        print(sentence)\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our data in the `unigram_sentences_all` file is now organized as a large text file with one sentence per line.  This format allows us to use gensim's LineSentence class, a convenient iterator for working with gensim's other components.  It *streams* the documents/sentences from disk, so you never have ot hold the entire corpus in RAM at once.  This allows you to scale the modeling to a very large corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bourbon and vanilla\n",
      " \n",
      "good breakfast ever\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 5,7):\n",
    "    print(u' '.join(unigram_sentence))\n",
    "    print(u' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_model_file = os.path.join(ngram_all, 'bigram_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1==1:\n",
    "    phrases = Phrases(unigram_sentences)\n",
    "    bigram_model = Phraser(phrases)\n",
    "    bigram_model.save(bigram_model_file)\n",
    "    \n",
    "bigram_model = Phrases.load(bigram_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = os.path.join(ngram_all,'bigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "\n",
    "    for unigram_sentence in unigram_sentences:\n",
    "\n",
    "        bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "\n",
    "        f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bourbon and vanilla\n",
      "\n",
      "good breakfast ever\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bigram_sentence in it.islice(bigram_sentences, 5, 7):\n",
    "    print(u' '.join(bigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join(ngram_all, 'trigram_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "\n",
    "    phrases_trigram = Phrases(bigram_sentences)\n",
    "    trigram_model = Phraser(phrases_trigram)\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1==1:\n",
    "    phrases = Phrases(unigram_sentences)\n",
    "    bigram_model = Phraser(phrases)\n",
    "    bigram_model.save(bigram_model_file)\n",
    "    \n",
    "bigram_model = Phrases.load(bigram_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = os.path.join(ngram_all, 'trigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "\n",
    "    for bigram_sentence in bigram_sentences:\n",
    "\n",
    "        trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "\n",
    "        f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tan head\n",
      "\n",
      "nose be perfect with huge maple syrup and coffee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 3, 5):\n",
    "    print(u' '.join(trigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_reviews_filepath = os.path.join(intermediate_directory,\n",
    "                                        'trigram_transformed_reviews_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(trigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "\n",
    "    for parsed_review in nlp.pipe(line_review(review_txt_filepath),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "\n",
    "        # lemmatize the text, removing punctuation and whitespace\n",
    "        unigram_review = [token.lemma_ for token in parsed_review\n",
    "                          if not punct_space(token)]\n",
    "\n",
    "        # apply the first-order and second-order phrase models\n",
    "        bigram_review = bigram_model[unigram_review]\n",
    "        trigram_review = trigram_model[bigram_review]\n",
    "\n",
    "        # remove any remaining stopwords\n",
    "        trigram_review = [term for term in trigram_review\n",
    "                          if term not in STOP_WORDS]\n",
    "        \n",
    "        # remove pronouns\n",
    "        trigram_review = [term for term in trigram_review\n",
    "                          if term !='-PRON-']\n",
    "        \n",
    "\n",
    "        # write the transformed review as a line in the new file\n",
    "        trigram_review = u' '.join(trigram_review)\n",
    "        f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "Black and thick. Tan head. Nose is perfect with huge maple syrup and coffee. Bourbon and vanilla. Best breakfast ever. Taste: maple vanilla bourbon and oak. Everything I want in a beer. Not overly roasted. Just perfect.Full bodied. Good carbonation. Ends with maple syrup.Amazing. That's all. Cannot believe it lives up to the hype.\n",
      "\n",
      "----\n",
      "\n",
      "Transformed:\n",
      "\n",
      "black thick tan head nose perfect huge maple syrup coffee bourbon vanilla good breakfast taste maple vanilla bourbon oak want beer overly roasted perfect bodied good carbonation end maple syrup amazing believe live hype\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(u'Original:' + u'\\n')\n",
    "\n",
    "for review in it.islice(line_review(review_txt_filepath), 2, 3):\n",
    "    print(review)\n",
    "\n",
    "print(u'----' + u'\\n')\n",
    "print(u'Transformed:' + u'\\n')\n",
    "\n",
    "with open(trigram_reviews_filepath, encoding='utf_8') as f:\n",
    "    for review in it.islice(f, 2, 3):\n",
    "        print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of *word vector embedding models*, or *word vector models* for short is to learn dense numerical representations of each term in a corpus vocabulary.  If the model is succesful, the vectors it learns should encode some information about the *meaning* or *concept* the term represents, and the relationship between it and other terms in the vocabulary.  Word vector models are fully unsupervised &mdash; they learn all of these meaning and relationships solely by analyzing the text of the corpus, without any advanced knowledge provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# run with or without stop words removed and word lemmatized\n",
    "trigram_sentences = LineSentence(trigram_reviews_filepath)\n",
    "# trigram_sentences = LineSentence(trigram_sentences_filepath)\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model for 5 epochs.\n"
     ]
    }
   ],
   "source": [
    "if 1==1:\n",
    "    \n",
    "    # initiate the model and perform one epoch of training\n",
    "    beer2vec = Word2Vec(trigram_sentences, size=100, window=5,\n",
    "                       min_count=1, sg=1) #workers=?\n",
    "    beer2vec.save(word2vec_filepath)\n",
    "    \n",
    "    #perform the next n epochs of training\n",
    "    for i in range(1, 5):\n",
    "        beer2vec.train(trigram_sentences, total_examples=beer2vec.corpus_count, epochs=2)\n",
    "        beer2vec.save(word2vec_filepath)\n",
    "        \n",
    "        \n",
    "#load the finished model from disk\n",
    "beer2vec = Word2Vec.load(word2vec_filepath)\n",
    "beer2vec.init_sims()\n",
    "\n",
    "print(f'Trained model for {beer2vec.train_count} epochs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>maple</th>\n",
       "      <td>0.103917</td>\n",
       "      <td>-0.140059</td>\n",
       "      <td>-0.017169</td>\n",
       "      <td>0.156164</td>\n",
       "      <td>-0.036451</td>\n",
       "      <td>-0.033113</td>\n",
       "      <td>-0.161625</td>\n",
       "      <td>-0.137750</td>\n",
       "      <td>0.025398</td>\n",
       "      <td>-0.162902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018641</td>\n",
       "      <td>0.129391</td>\n",
       "      <td>0.137875</td>\n",
       "      <td>0.123555</td>\n",
       "      <td>0.062780</td>\n",
       "      <td>-0.134537</td>\n",
       "      <td>-0.022336</td>\n",
       "      <td>0.073853</td>\n",
       "      <td>0.097382</td>\n",
       "      <td>0.031955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bourbon</th>\n",
       "      <td>-0.097405</td>\n",
       "      <td>0.136079</td>\n",
       "      <td>-0.082729</td>\n",
       "      <td>-0.028771</td>\n",
       "      <td>0.161088</td>\n",
       "      <td>0.041544</td>\n",
       "      <td>0.122417</td>\n",
       "      <td>0.111509</td>\n",
       "      <td>-0.151671</td>\n",
       "      <td>0.112550</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102508</td>\n",
       "      <td>-0.045750</td>\n",
       "      <td>-0.096369</td>\n",
       "      <td>0.116023</td>\n",
       "      <td>0.131989</td>\n",
       "      <td>0.027987</td>\n",
       "      <td>0.030419</td>\n",
       "      <td>-0.012543</td>\n",
       "      <td>-0.009270</td>\n",
       "      <td>-0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black</th>\n",
       "      <td>-0.016077</td>\n",
       "      <td>0.040062</td>\n",
       "      <td>-0.165585</td>\n",
       "      <td>-0.127946</td>\n",
       "      <td>-0.110090</td>\n",
       "      <td>0.039310</td>\n",
       "      <td>-0.027393</td>\n",
       "      <td>-0.106662</td>\n",
       "      <td>0.022707</td>\n",
       "      <td>-0.070321</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099313</td>\n",
       "      <td>0.066397</td>\n",
       "      <td>-0.136136</td>\n",
       "      <td>-0.091080</td>\n",
       "      <td>-0.070751</td>\n",
       "      <td>0.137367</td>\n",
       "      <td>-0.009309</td>\n",
       "      <td>-0.069311</td>\n",
       "      <td>-0.179939</td>\n",
       "      <td>-0.170126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>head</th>\n",
       "      <td>-0.008093</td>\n",
       "      <td>-0.159113</td>\n",
       "      <td>-0.100159</td>\n",
       "      <td>-0.045215</td>\n",
       "      <td>0.133857</td>\n",
       "      <td>0.142241</td>\n",
       "      <td>-0.142798</td>\n",
       "      <td>-0.021481</td>\n",
       "      <td>-0.012815</td>\n",
       "      <td>0.087705</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043162</td>\n",
       "      <td>0.074166</td>\n",
       "      <td>-0.154252</td>\n",
       "      <td>0.138262</td>\n",
       "      <td>-0.061717</td>\n",
       "      <td>0.021467</td>\n",
       "      <td>0.115386</td>\n",
       "      <td>0.034662</td>\n",
       "      <td>-0.158987</td>\n",
       "      <td>-0.018623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vanilla</th>\n",
       "      <td>0.065545</td>\n",
       "      <td>0.036689</td>\n",
       "      <td>-0.018900</td>\n",
       "      <td>0.181729</td>\n",
       "      <td>-0.140313</td>\n",
       "      <td>-0.076830</td>\n",
       "      <td>0.086769</td>\n",
       "      <td>-0.119873</td>\n",
       "      <td>0.144033</td>\n",
       "      <td>-0.134409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152219</td>\n",
       "      <td>-0.078176</td>\n",
       "      <td>-0.104161</td>\n",
       "      <td>0.056544</td>\n",
       "      <td>-0.106982</td>\n",
       "      <td>-0.052897</td>\n",
       "      <td>-0.129495</td>\n",
       "      <td>-0.145600</td>\n",
       "      <td>0.081123</td>\n",
       "      <td>-0.109839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfect</th>\n",
       "      <td>0.061904</td>\n",
       "      <td>-0.055954</td>\n",
       "      <td>0.028688</td>\n",
       "      <td>-0.074083</td>\n",
       "      <td>0.048087</td>\n",
       "      <td>0.034456</td>\n",
       "      <td>-0.045856</td>\n",
       "      <td>-0.132805</td>\n",
       "      <td>-0.020339</td>\n",
       "      <td>-0.035591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.148506</td>\n",
       "      <td>0.099832</td>\n",
       "      <td>-0.174569</td>\n",
       "      <td>-0.047459</td>\n",
       "      <td>-0.124500</td>\n",
       "      <td>-0.160330</td>\n",
       "      <td>0.082956</td>\n",
       "      <td>0.135434</td>\n",
       "      <td>-0.082111</td>\n",
       "      <td>-0.004731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syrup</th>\n",
       "      <td>-0.030821</td>\n",
       "      <td>-0.009972</td>\n",
       "      <td>-0.070644</td>\n",
       "      <td>0.105364</td>\n",
       "      <td>-0.127893</td>\n",
       "      <td>-0.033354</td>\n",
       "      <td>-0.022315</td>\n",
       "      <td>-0.147738</td>\n",
       "      <td>0.035417</td>\n",
       "      <td>-0.092762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137432</td>\n",
       "      <td>-0.048187</td>\n",
       "      <td>-0.054458</td>\n",
       "      <td>0.103897</td>\n",
       "      <td>0.063666</td>\n",
       "      <td>-0.104626</td>\n",
       "      <td>0.116045</td>\n",
       "      <td>-0.109428</td>\n",
       "      <td>-0.164639</td>\n",
       "      <td>-0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>0.093007</td>\n",
       "      <td>-0.091432</td>\n",
       "      <td>-0.139140</td>\n",
       "      <td>0.163025</td>\n",
       "      <td>0.140034</td>\n",
       "      <td>-0.018149</td>\n",
       "      <td>-0.091858</td>\n",
       "      <td>-0.066397</td>\n",
       "      <td>-0.120680</td>\n",
       "      <td>0.124626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088865</td>\n",
       "      <td>-0.145545</td>\n",
       "      <td>0.041684</td>\n",
       "      <td>-0.036811</td>\n",
       "      <td>-0.095805</td>\n",
       "      <td>0.108726</td>\n",
       "      <td>-0.115966</td>\n",
       "      <td>-0.140369</td>\n",
       "      <td>0.066940</td>\n",
       "      <td>0.024430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>0.134035</td>\n",
       "      <td>0.043455</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>-0.014036</td>\n",
       "      <td>0.107356</td>\n",
       "      <td>0.101851</td>\n",
       "      <td>0.065626</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.033015</td>\n",
       "      <td>0.017671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065428</td>\n",
       "      <td>0.123413</td>\n",
       "      <td>-0.094405</td>\n",
       "      <td>-0.141310</td>\n",
       "      <td>0.111107</td>\n",
       "      <td>0.057776</td>\n",
       "      <td>0.065624</td>\n",
       "      <td>0.131169</td>\n",
       "      <td>-0.171928</td>\n",
       "      <td>-0.019020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beer</th>\n",
       "      <td>-0.101112</td>\n",
       "      <td>0.042286</td>\n",
       "      <td>-0.050635</td>\n",
       "      <td>-0.056913</td>\n",
       "      <td>0.048862</td>\n",
       "      <td>-0.023690</td>\n",
       "      <td>-0.134458</td>\n",
       "      <td>0.036281</td>\n",
       "      <td>0.047062</td>\n",
       "      <td>-0.032511</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072308</td>\n",
       "      <td>-0.028197</td>\n",
       "      <td>0.011054</td>\n",
       "      <td>-0.039439</td>\n",
       "      <td>-0.100773</td>\n",
       "      <td>-0.179719</td>\n",
       "      <td>-0.102104</td>\n",
       "      <td>-0.153850</td>\n",
       "      <td>-0.107923</td>\n",
       "      <td>0.009471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6   \\\n",
       "maple    0.103917 -0.140059 -0.017169  0.156164 -0.036451 -0.033113 -0.161625   \n",
       "bourbon -0.097405  0.136079 -0.082729 -0.028771  0.161088  0.041544  0.122417   \n",
       "black   -0.016077  0.040062 -0.165585 -0.127946 -0.110090  0.039310 -0.027393   \n",
       "head    -0.008093 -0.159113 -0.100159 -0.045215  0.133857  0.142241 -0.142798   \n",
       "vanilla  0.065545  0.036689 -0.018900  0.181729 -0.140313 -0.076830  0.086769   \n",
       "perfect  0.061904 -0.055954  0.028688 -0.074083  0.048087  0.034456 -0.045856   \n",
       "syrup   -0.030821 -0.009972 -0.070644  0.105364 -0.127893 -0.033354 -0.022315   \n",
       "good     0.093007 -0.091432 -0.139140  0.163025  0.140034 -0.018149 -0.091858   \n",
       "want     0.134035  0.043455  0.126700 -0.014036  0.107356  0.101851  0.065626   \n",
       "beer    -0.101112  0.042286 -0.050635 -0.056913  0.048862 -0.023690 -0.134458   \n",
       "\n",
       "               7         8         9     ...           90        91        92  \\\n",
       "maple   -0.137750  0.025398 -0.162902    ...     0.018641  0.129391  0.137875   \n",
       "bourbon  0.111509 -0.151671  0.112550    ...    -0.102508 -0.045750 -0.096369   \n",
       "black   -0.106662  0.022707 -0.070321    ...    -0.099313  0.066397 -0.136136   \n",
       "head    -0.021481 -0.012815  0.087705    ...    -0.043162  0.074166 -0.154252   \n",
       "vanilla -0.119873  0.144033 -0.134409    ...     0.152219 -0.078176 -0.104161   \n",
       "perfect -0.132805 -0.020339 -0.035591    ...    -0.148506  0.099832 -0.174569   \n",
       "syrup   -0.147738  0.035417 -0.092762    ...     0.137432 -0.048187 -0.054458   \n",
       "good    -0.066397 -0.120680  0.124626    ...    -0.088865 -0.145545  0.041684   \n",
       "want     0.004171  0.033015  0.017671    ...     0.065428  0.123413 -0.094405   \n",
       "beer     0.036281  0.047062 -0.032511    ...    -0.072308 -0.028197  0.011054   \n",
       "\n",
       "               93        94        95        96        97        98        99  \n",
       "maple    0.123555  0.062780 -0.134537 -0.022336  0.073853  0.097382  0.031955  \n",
       "bourbon  0.116023  0.131989  0.027987  0.030419 -0.012543 -0.009270 -0.000060  \n",
       "black   -0.091080 -0.070751  0.137367 -0.009309 -0.069311 -0.179939 -0.170126  \n",
       "head     0.138262 -0.061717  0.021467  0.115386  0.034662 -0.158987 -0.018623  \n",
       "vanilla  0.056544 -0.106982 -0.052897 -0.129495 -0.145600  0.081123 -0.109839  \n",
       "perfect -0.047459 -0.124500 -0.160330  0.082956  0.135434 -0.082111 -0.004731  \n",
       "syrup    0.103897  0.063666 -0.104626  0.116045 -0.109428 -0.164639 -0.111111  \n",
       "good    -0.036811 -0.095805  0.108726 -0.115966 -0.140369  0.066940  0.024430  \n",
       "want    -0.141310  0.111107  0.057776  0.065624  0.131169 -0.171928 -0.019020  \n",
       "beer    -0.039439 -0.100773 -0.179719 -0.102104 -0.153850 -0.107923  0.009471  \n",
       "\n",
       "[10 rows x 100 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# build a list of the terms, integer indices,\n",
    "# and term counts from the food2vec model vocabulary\n",
    "ordered_vocab = [(term, voc.index, voc.count)\n",
    "                 for term, voc in beer2vec.wv.vocab.items()]\n",
    "\n",
    "# sort by the term counts, so the most common terms appear first\n",
    "ordered_vocab = sorted(ordered_vocab, key=lambda tup: -1*tup[-1])\n",
    "\n",
    "# # unzip the terms, integer indices, and counts into separate lists\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "\n",
    "# # create a DataFrame with the food2vec vectors as data,\n",
    "# # and the terms as row labels\n",
    "word_vectors = pd.DataFrame(beer2vec.wv.syn0norm[term_indices, :],\n",
    "                            index=ordered_terms)\n",
    "\n",
    "word_vectors.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
