{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivo Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for a new way to compare and recommend beer.  \n",
    "\n",
    "This notebook scrapes beer reviews from [BeerAdvocate](https://www.beeradvocate.com/) then performs natural language processing on these reviews.  Once a profile of a beer has been created, a similar, semi-similar, or completely different beer can be recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the scrapy scraper (and generate all the data required for this analysis) follow these steps:\n",
    " 1. move into the beerAdvocateScraper directory (cd beerAdvocateScraper)\n",
    " 2. execute the scraper (scrapy crawl reviewScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %cd beerAdvocateScraper\n",
    "# %scrapy crawl reviewScraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and load the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrase modeling is an approach to learning combinations of tokens that together represent meaning multi-word concepts.  These phrase models are developed by looping over the words in the corpus and finding words that appear together more than they should by random chance.  The formula used to determine if whether two tokens $A$ and $B$ constitute a phrase is:\n",
    "\n",
    "$$\n",
    "\\frac {count(A B) - count_{min}}\n",
    "{count(A) * count(B)}\n",
    "*N > threshold\n",
    "$$\n",
    "\n",
    "where:\n",
    "* $count(A)$ is the number of times $A$ appears in the corpus\n",
    "* $count(B)$ is the number of times $B$ appears in the corpus\n",
    "* $count(AB)$ is the number of times $AB$ appear in the corpus in that order\n",
    "* $N$ is the total size of the corpus vocabulary\n",
    "* $count_{min}$ is a user-defined parameter to ensure that the phrase appears a minimum number of times\n",
    "* $threshold$ is a user-defined paramter to control how strong the relationship must be before the two tokens are considered a single concept\n",
    "\n",
    "Once we have trained the phrase model we can apply it to the reviews in our corpus.  It will consider the multiworded tokens to be single phrases.\n",
    "\n",
    "The gensim library will help us with phrase modeling, specifically the Phrases class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mv beerAdvocateScraper/BeerAdvocateReviews.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating slimReview.csv for testing purposes\n",
    "\n",
    "!head -20 BeerAdvocateReviews.csv >> slimReviews.csv\n",
    "!tail -20 BeerAdvocateReviews.csv >> slimReviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%mkdir ./intermediate\n",
    "%mv BeerAdvocateReviews.csv ./intermediate/\n",
    "%mv slimReviews.csv ./intermediate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile('^[\\d,\"]+')\n",
    "\n",
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            try:\n",
    "                yield re.split(pattern, review)[1].replace('\\\\n', '\\n')\n",
    "            except: pass\n",
    "            if count % 1000 == 0:\n",
    "                print(f'on review {count}')\n",
    "            count += 1\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=1000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intermediate_directory = os.path.join('.', 'intermediate')\n",
    "# review_txt_filepath = os.path.join(intermediate_directory,'BeerAdvocateReviews.csv')\n",
    "review_txt_filepath = os.path.join(intermediate_directory,'slimReviews.csv')\n",
    "\n",
    "%mkdir './intermediate/ngram_all'\n",
    "ngram_all = os.path.join(intermediate_directory, 'ngram_all')\n",
    "unigram_sentences_filepath = os.path.join(ngram_all,\n",
    "                                          'unigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `lemmatized_sentence_corpus` generator to loop over the original review text, segmenting the reviews into individual sentences and normalizing the text.  We will write this data back out to a new file (`unigram_sentence_all`), with one normalized sentence per line.  We will use this data for learning our phrase models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "    for sentence in lemmatized_sentence_corpus(review_txt_filepath):\n",
    "#         print(sentence)\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data in the `unigram_sentences_all` file is now organized as a large text file with one sentence per line.  This format allows us to use gensim's LineSentence class, a convenient iterator for working with gensim's other components.  It *streams* the documents/sentences from disk, so you never have to hold the entire corpus in RAM at once.  This allows you to scale the modeling to a very large corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 5,7):\n",
    "    print(u' '.join(unigram_sentence))\n",
    "    print(u' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_model_file = os.path.join(ngram_all, 'bigram_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1==1:\n",
    "    phrases = Phrases(unigram_sentences)\n",
    "    bigram_model = Phraser(phrases)\n",
    "    bigram_model.save(bigram_model_file)\n",
    "    \n",
    "bigram_model = Phrases.load(bigram_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = os.path.join(ngram_all,'bigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "\n",
    "    for unigram_sentence in unigram_sentences:\n",
    "\n",
    "        bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "\n",
    "        f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bigram_sentence in it.islice(bigram_sentences, 5, 7):\n",
    "    print(u' '.join(bigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join(ngram_all, 'trigram_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "\n",
    "    phrases_trigram = Phrases(bigram_sentences)\n",
    "    trigram_model = Phraser(phrases_trigram)\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1==1:\n",
    "    phrases = Phrases(unigram_sentences)\n",
    "    bigram_model = Phraser(phrases)\n",
    "    bigram_model.save(bigram_model_file)\n",
    "    \n",
    "bigram_model = Phrases.load(bigram_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = os.path.join(ngram_all, 'trigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "\n",
    "    for bigram_sentence in bigram_sentences:\n",
    "\n",
    "        trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "\n",
    "        f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 3, 5):\n",
    "    print(u' '.join(trigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_reviews_filepath = os.path.join(intermediate_directory,\n",
    "                                        'trigram_transformed_reviews_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(trigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "\n",
    "    for parsed_review in nlp.pipe(line_review(review_txt_filepath),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "\n",
    "        # lemmatize the text, removing punctuation and whitespace\n",
    "        unigram_review = [token.lemma_ for token in parsed_review\n",
    "                          if not punct_space(token)]\n",
    "\n",
    "        # apply the first-order and second-order phrase models\n",
    "        bigram_review = bigram_model[unigram_review]\n",
    "        trigram_review = trigram_model[bigram_review]\n",
    "\n",
    "        # remove any remaining stopwords\n",
    "        trigram_review = [term for term in trigram_review\n",
    "                          if term not in STOP_WORDS]\n",
    "        \n",
    "        # remove pronouns\n",
    "        trigram_review = [term for term in trigram_review\n",
    "                          if term !='-PRON-']\n",
    "        \n",
    "\n",
    "        # write the transformed review as a line in the new file\n",
    "        trigram_review = u' '.join(trigram_review)\n",
    "        f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u'Original:' + u'\\n')\n",
    "\n",
    "for review in it.islice(line_review(review_txt_filepath), 4, 5):\n",
    "    print(review)\n",
    "\n",
    "print(u'----' + u'\\n')\n",
    "print(u'Transformed:' + u'\\n')\n",
    "\n",
    "with open(trigram_reviews_filepath, encoding='utf_8') as f:\n",
    "    for review in it.islice(f, 4, 5):\n",
    "        print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beerID(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    with open(review_txt_filepath, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            try:\n",
    "                yield re.split('(^[\\d\"]+)', review)[1].replace('\\\\n', '\\n')\n",
    "            except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beerDict = {}\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    with open(trigram_reviews_filepath, encoding='utf_8') as f:\n",
    "        for review in it.islice(f, i, i+1):\n",
    "            beerID = list(it.islice(get_beerID(review_txt_filepath), i, i+1))[0]\n",
    "            wordDict = Counter()\n",
    "            for word in review.split():\n",
    "                wordDict[word] = wordDict.get(word,0) + 1\n",
    "#             print(wordDict)\n",
    "#             print('---------------')\n",
    "            beerDict[beerID] = beerDict.get(beerID, Counter()) + wordDict\n",
    "print(beerDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pd.read_csv(review_txt_filepath)\n",
    "\n",
    "beerDict = {}\n",
    "\n",
    "for i in range(m.shape[0]):\n",
    "    with open(trigram_reviews_filepath, encoding='utf_8') as f:\n",
    "        for review in it.islice(f, i, i+1):\n",
    "            beerID = list(it.islice(get_beerID(review_txt_filepath), i, i+1))[0]\n",
    "            wordDict = Counter()\n",
    "            for word in review.split():\n",
    "                wordDict[word] = wordDict.get(word,0) + 1\n",
    "            beerDict[beerID] = beerDict.get(beerID, Counter()) + wordDict\n",
    "# print(beerDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any items in beerDict with less than n occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beerDict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of *word vector embedding models*, or *word vector models* for short is to learn dense numerical representations of each term in a corpus vocabulary.  If the model is succesful, the vectors it learns should encode some information about the *meaning* or *concept* the term represents, and the relationship between it and other terms in the vocabulary.  Word vector models are fully unsupervised &mdash; they learn all of these meaning and relationships solely by analyzing the text of the corpus, without any advanced knowledge provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# run with or without stop words removed and word lemmatized\n",
    "trigram_sentences = LineSentence(trigram_reviews_filepath)\n",
    "# trigram_sentences = LineSentence(trigram_sentences_filepath)\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==1:\n",
    "    \n",
    "    # initiate the model and perform one epoch of training\n",
    "    beer2vec = Word2Vec(trigram_sentences, size=100, window=5,\n",
    "                       min_count=1, sg=1) #workers=?\n",
    "    beer2vec.save(word2vec_filepath)\n",
    "    \n",
    "    #perform the next n epochs of training\n",
    "    for i in range(1, 5):\n",
    "        beer2vec.train(trigram_sentences, total_examples=beer2vec.corpus_count, epochs=2)\n",
    "        beer2vec.save(word2vec_filepath)\n",
    "        \n",
    "        \n",
    "#load the finished model from disk\n",
    "beer2vec = Word2Vec.load(word2vec_filepath)\n",
    "beer2vec.init_sims()\n",
    "\n",
    "print(f'Trained model for {beer2vec.train_count} epochs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build a list of the terms, integer indices,\n",
    "# and term counts from the food2vec model vocabulary\n",
    "ordered_vocab = [(term, voc.index, voc.count)\n",
    "                 for term, voc in beer2vec.wv.vocab.items()]\n",
    "\n",
    "# sort by the term counts, so the most common terms appear first\n",
    "ordered_vocab = sorted(ordered_vocab, key=lambda tup: -1*tup[-1])\n",
    "\n",
    "# # unzip the terms, integer indices, and counts into separate lists\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "\n",
    "# # create a DataFrame with the food2vec vectors as data,\n",
    "# # and the terms as row labels\n",
    "word_vectors = pd.DataFrame(beer2vec.wv.syn0norm[term_indices, :],\n",
    "                            index=ordered_terms)\n",
    "\n",
    "word_vectors.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction Using t-SNE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-Distributed Stochastic Neighbor Embedding, or *t-SNE*, is a dimensionality reduction technique to assist with visualizing high-dimensional datasets.  It attempts to map high-dimensional data onto a low (2 or 3) dimensional  representation such that the relative distance between points are preserved as closely as possible in both high and low dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "tsneInput = word_vectors.drop(STOP_WORDS, errors=u'ignore')\n",
    "tsneInput = tsneInput.head(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsneInput.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_filepath = os.path.join(intermediate_directory,\n",
    "                             u'tsne_model')\n",
    "\n",
    "tsne_vectors_filepath = os.path.join(intermediate_directory,\n",
    "                                     u'tsne_vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsneInput.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "    \n",
    "    tsne = TSNE()\n",
    "    tsne_vectors = tsne.fit_transform(tsneInput.values)\n",
    "    \n",
    "    with open(tsne_filepath, 'wb') as f:\n",
    "        pickle.dump(tsne, f)\n",
    "\n",
    "    pd.np.save(tsne_vectors_filepath, tsne_vectors)\n",
    "    \n",
    "with open(tsne_filepath, 'rb') as f:\n",
    "    tsne = pickle.load(f)\n",
    "    \n",
    "tsne_vectors = pd.np.load(tsne_vectors_filepath)\n",
    "\n",
    "tsne_vectors = pd.DataFrame(tsne_vectors,\n",
    "                            index=pd.Index(tsneInput.index),\n",
    "                            columns=[u'x_coord', u'y_coord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_vectors[u'word'] = tsne_vectors.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(beerDict.keys())\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = []\n",
    "for i in tsne_vectors.index:\n",
    "    list1.append(i + '_x')\n",
    "    list1.append(i + '_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDF = pd.DataFrame(columns=list1, index=ids)\n",
    "masterDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in beerDict.items():\n",
    "    for k2, v2 in value.items():\n",
    "        try:\n",
    "            x_val = tsne_vectors.loc[k2]['x_coord'] * v2\n",
    "            columnLookupX = k2 + '_x'\n",
    "            masterDF.loc[key][columnLookupX] = x_val\n",
    "\n",
    "            y_val = tsne_vectors.loc[k2]['y_coord'] * v2\n",
    "            columnLookupY = k2 + '_y'\n",
    "            masterDF.loc[key][columnLookupY] = y_val\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDF.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(masterDF.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in combinations(list(masterDF.index), 2):\n",
    "    cs = 1 - spatial.distance.cosine(masterDF.loc[pair[0]].values.reshape(-1,1), masterDF.loc[pair[1]].values.reshape(-1,1))\n",
    "    print(f'The similarity between {pair[0]} and {pair[1]} is {cs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the similarity between every beer pair\n",
    "# create a dictionary where the key is the beer and value is the list of beers from most to least similar"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
