{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run scraper.py to scrap data and save it to review_text_all.txt\n",
    "import scraper as s\n",
    "s.runScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Phrase Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrase modeling is an approach to learning combinations of tokens that together represent meaning multi-word concepts.  These phrase models are developed by looping over the words in the corpus and finding words that appear together more than they should by random chance.  The formula used to determine if whether two tokens $A$ and $B$ constitute a phrase is:\n",
    "\n",
    "$$\n",
    "\\frac {count(A B) - count_{min}}\n",
    "{count(A) * count(B)}\n",
    "*N > threshold\n",
    "$$\n",
    "\n",
    "where:\n",
    "* $count(A)$ is the number of times $A$ appears in the corpus\n",
    "* $count(B)$ is the number of times $B$ appears in the corpus\n",
    "* $count(AB)$ is the number of times $AB$ appear in the corpus in that order\n",
    "* $N$ is the total size of the corpus vocabulary\n",
    "* $count_{min}$ is a user-defined parameter to ensure that the phrase appears a minimum number of times\n",
    "* $threshold$ is a user-defined paramter to control how strong the relationship must be before the two tokens are considered a single concept\n",
    "\n",
    "Once we have trained the phrase model we can apply it to the reviews in our corpus.  It will consider the multiworded tokens to be single phrases.\n",
    "\n",
    "The gensim library will help us with phrase modeling, specifically the Phrases class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%mv beerAdvocateScraper/beerAdvocateScraper/BeerAdvocateReviews.csv ./intermediate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.split(',')[1].replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intermediate_directory = os.path.join('.', 'intermediate')\n",
    "review_txt_filepath = os.path.join(intermediate_directory,'BeerAdvocateReviews.csv')\n",
    "ngram_all = os.path.join(intermediate_directory, 'ngram_all')\n",
    "unigram_sentences_filepath = os.path.join(ngram_all,\n",
    "                                          'unigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `lemmatized_sentence_corpus` generator to loop over the original review text, segmenting the reviews into individual sentences and normalizing the text.  We will write this data back out to a new file (`unigram_sentence_all`), with one normalized sentence per line.  We will use this data for learning our phrase models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewtext\n",
      "2014 vintage\n",
      "black and thick\n",
      "tan head\n",
      "nose be perfect with huge maple syrup and coffee\n",
      "bourbon and vanilla\n",
      "good breakfast ever\n",
      "taste maple vanilla bourbon and oak\n",
      "everything -PRON- want in a beer\n",
      "not overly roasted\n",
      "just perfect\n",
      "full bodied\n",
      "good carbonation\n",
      "end with maple syrup\n",
      "amazing\n",
      "that be all\n",
      "can not believe -PRON- live up to the hype\n",
      "12 oz\n",
      "bottle pour into a snifter\n",
      "no bottle number\n",
      "appearance inky black\n",
      "stain the glass again\n",
      "big and dark head than mornin delight and assassin\n",
      "great surface wisps\n",
      "smell bourbon\n",
      "maple\n",
      "vanilla\n",
      "dark chocolate\n",
      "everything -PRON- would want in a stout\n",
      "the bourbon do take over\n",
      "serve alongside mornin delight and assassin\n",
      "the beer be near black with a filmy beige head\n",
      "all hype and bias aside\n"
     ]
    }
   ],
   "source": [
    "with open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "    for sentence in lemmatized_sentence_corpus(review_txt_filepath):\n",
    "        print(sentence)\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our data in the `unigram_sentences_all` file is now organized as a large text file with one sentence per line.  This format allows us to use gensim's LineSentence class, a convenient iterator for working with gensim's other components.  It *streams* the documents/sentences from disk, so you never have ot hold the entire corpus in RAM at once.  This allows you to scale the modeling to a very large corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bourbon and vanilla\n",
      " \n",
      "good breakfast ever\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 5,7):\n",
    "    print(u' '.join(unigram_sentence))\n",
    "    print(u' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_model_file = os.path.join(ngram_all, 'bigram_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1==1:\n",
    "    phrases = Phrases(unigram_sentences)\n",
    "    bigram_model = Phraser(phrases)\n",
    "    bigram_model.save(bigram_model_file)\n",
    "    \n",
    "bigram_model = Phrases.load(bigram_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = os.path.join(ngram_all,'bigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "\n",
    "    for unigram_sentence in unigram_sentences:\n",
    "\n",
    "        bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "\n",
    "        f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bourbon and vanilla\n",
      "\n",
      "good breakfast ever\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bigram_sentence in it.islice(bigram_sentences, 5, 7):\n",
    "    print(u' '.join(bigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join(ngram_all, 'trigram_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "\n",
    "    phrases_trigram = Phrases(bigram_sentences)\n",
    "    trigram_model = Phraser(phrases_trigram)\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1==1:\n",
    "    phrases = Phrases(unigram_sentences)\n",
    "    bigram_model = Phraser(phrases)\n",
    "    bigram_model.save(bigram_model_file)\n",
    "    \n",
    "bigram_model = Phrases.load(bigram_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = os.path.join(ngram_all, 'trigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "\n",
    "    for bigram_sentence in bigram_sentences:\n",
    "\n",
    "        trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "\n",
    "        f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tan head\n",
      "\n",
      "nose be perfect with huge maple syrup and coffee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 3, 5):\n",
    "    print(u' '.join(trigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_reviews_filepath = os.path.join(intermediate_directory,\n",
    "                                        'trigram_transformed_reviews_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(trigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "\n",
    "    for parsed_review in nlp.pipe(line_review(review_txt_filepath),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "\n",
    "        # lemmatize the text, removing punctuation and whitespace\n",
    "        unigram_review = [token.lemma_ for token in parsed_review\n",
    "                          if not punct_space(token)]\n",
    "\n",
    "        # apply the first-order and second-order phrase models\n",
    "        bigram_review = bigram_model[unigram_review]\n",
    "        trigram_review = trigram_model[bigram_review]\n",
    "\n",
    "        # remove any remaining stopwords\n",
    "        trigram_review = [term for term in trigram_review\n",
    "                          if term not in STOP_WORDS]\n",
    "        \n",
    "        # remove pronouns\n",
    "        trigram_review = [term for term in trigram_review\n",
    "                          if term !='-PRON-']\n",
    "        \n",
    "\n",
    "        # write the transformed review as a line in the new file\n",
    "        trigram_review = u' '.join(trigram_review)\n",
    "        f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "Black and thick. Tan head. Nose is perfect with huge maple syrup and coffee. Bourbon and vanilla. Best breakfast ever. Taste: maple vanilla bourbon and oak. Everything I want in a beer. Not overly roasted. Just perfect.Full bodied. Good carbonation. Ends with maple syrup.Amazing. That's all. Cannot believe it lives up to the hype.\n",
      "\n",
      "----\n",
      "\n",
      "Transformed:\n",
      "\n",
      "black thick tan head nose perfect huge maple syrup coffee bourbon vanilla good breakfast taste maple vanilla bourbon oak want beer overly roasted perfect bodied good carbonation end maple syrup amazing believe live hype\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(u'Original:' + u'\\n')\n",
    "\n",
    "for review in it.islice(line_review(review_txt_filepath), 2, 3):\n",
    "    print(review)\n",
    "\n",
    "print(u'----' + u'\\n')\n",
    "print(u'Transformed:' + u'\\n')\n",
    "\n",
    "with open(trigram_reviews_filepath, encoding='utf_8') as f:\n",
    "    for review in it.islice(f, 2, 3):\n",
    "        print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of *word vector embedding models*, or *word vector models* for short is to learn dense numerical representations of each term in a corpus vocabulary.  If the model is succesful, the vectors it learns should encode some information about the *meaning* or *concept* the term represents, and the relationship between it and other terms in the vocabulary.  Word vector models are fully unsupervised &mdash; they learn all of these meaning and relationships solely by analyzing the text of the corpus, without any advanced knowledge provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "trigram_sentences = LineSentence(trigram_sentences_filepath)\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model for 5 epochs.\n"
     ]
    }
   ],
   "source": [
    "if 1==1:\n",
    "    \n",
    "    # initiate the model and perform one epoch of training\n",
    "    beer2vec = Word2Vec(trigram_sentences, size=100, window=5,\n",
    "                       min_count=1, sg=1) #workers=?\n",
    "    beer2vec.save(word2vec_filepath)\n",
    "    \n",
    "    #perform the next n epochs of training\n",
    "    for i in range(1, 5):\n",
    "        beer2vec.train(trigram_sentences, total_examples=beer2vec.corpus_count, epochs=2)\n",
    "        beer2vec.save(word2vec_filepath)\n",
    "        \n",
    "        \n",
    "#load the finished model from disk\n",
    "beer2vec = Word2Vec.load(word2vec_filepath)\n",
    "beer2vec.init_sims()\n",
    "\n",
    "print(f'Trained model for {beer2vec.train_count} epochs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a list of the terms, integer indices,\n",
    "# and term counts from the food2vec model vocabulary\n",
    "\n",
    "len(beer2vec.wv['tan'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
